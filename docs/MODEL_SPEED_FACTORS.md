# C√°c Y·∫øu T·ªë ·∫¢nh H∆∞·ªüng ƒê·∫øn T·ªëc ƒê·ªô Model

## C√¢u H·ªèi: T·ªëc ƒë·ªô model c√≥ ph·ª• thu·ªôc v√†o ch√≠nh model kh√¥ng?

**Tr·∫£ l·ªùi: C√ì** - T·ªëc ƒë·ªô ph·ª• thu·ªôc v√†o **C·∫¢ model V√Ä prompt**, nh∆∞ng **model l√† y·∫øu t·ªë quan tr·ªçng nh·∫•t**.

---

## 1. Y·∫øu T·ªë Model (·∫¢nh H∆∞·ªüng L·ªõn Nh·∫•t) üî•

### 1.1. K√≠ch Th∆∞·ªõc Model
- **Model nh·ªè** (7B params): ~10-50 tokens/gi√¢y
- **Model v·ª´a** (13B params): ~5-20 tokens/gi√¢y  
- **Model l·ªõn** (70B params): ~1-5 tokens/gi√¢y

**V√≠ d·ª• trong code:**
- `llama3.2` (3B) ‚Üí Nhanh
- `llama3.1:70b` ‚Üí Ch·∫≠m h∆°n 20-30 l·∫ßn

### 1.2. Ki·∫øn Tr√∫c Model
- **Encoder-only** (BERT, BGE-M3): Nhanh, x·ª≠ l√Ω song song
- **Decoder-only** (GPT, Llama): Ch·∫≠m h∆°n, generate tu·∫ßn t·ª±
- **Encoder-Decoder** (T5, BART): Trung b√¨nh

**Trong code hi·ªán t·∫°i:**
```python
# Embedding model (encoder-only) - NHANH
embedding_service.encode_single(query)  # ~50-200ms

# LLM (decoder-only) - CH·∫¨M H∆†N
llm_service.generate(messages)  # ~1-10 gi√¢y
```

### 1.3. T·ªëi ∆Øu H√≥a Model
- **Full precision** (FP32): Ch·∫≠m nh·∫•t, ch√≠nh x√°c nh·∫•t
- **Half precision** (FP16): Nhanh h∆°n 2x, gi·∫£m VRAM 50%
- **Quantization** (Q4, Q8): Nhanh h∆°n 3-5x, gi·∫£m VRAM 75-90%

**V√≠ d·ª• Ollama:**
```bash
ollama run llama3.2        # FP16, ~2GB VRAM
ollama run llama3.2:q4_0   # Q4, ~1GB VRAM, nhanh h∆°n 2-3x
```

---

## 2. Y·∫øu T·ªë Prompt (·∫¢nh H∆∞·ªüng Trung B√¨nh)

### 2.1. ƒê·ªô D√†i Prompt
- **Prompt ng·∫Øn** (100 tokens): X·ª≠ l√Ω nhanh
- **Prompt d√†i** (2000 tokens): X·ª≠ l√Ω ch·∫≠m h∆°n 20x

**Trong code:**
```python
# Prompt ng·∫Øn
query = "What is AI?"  # ~5 tokens ‚Üí Nhanh

# Prompt d√†i (c√≥ context)
context_text = "...1000 words..."  # ~250 tokens ‚Üí Ch·∫≠m h∆°n
```

### 2.2. ƒê·ªô Ph·ª©c T·∫°p
- **C√¢u h·ªèi ƒë∆°n gi·∫£n**: "What is X?" ‚Üí Nhanh
- **C√¢u h·ªèi ph·ª©c t·∫°p**: "Explain X, compare with Y, analyze Z..." ‚Üí Ch·∫≠m h∆°n

### 2.3. S·ªë L∆∞·ª£ng Context
- **Kh√¥ng c√≥ context**: Nhanh nh·∫•t
- **5 contexts** (top_k=5): Ch·∫≠m h∆°n ~2-3x
- **20 contexts**: Ch·∫≠m h∆°n ~5-10x

---

## 3. Y·∫øu T·ªë Hardware (·∫¢nh H∆∞·ªüng L·ªõn) üíª

### 3.1. GPU vs CPU
- **GPU (CUDA)**: Nhanh h∆°n CPU 10-100x
- **CPU**: Ch·∫≠m, ch·ªâ d√πng khi kh√¥ng c√≥ GPU

**Trong code:**
```python
# Embedding service
device = os.getenv("EMBEDDING_DEVICE", "cuda")  # GPU ‚Üí Nhanh
# CPU ‚Üí Ch·∫≠m h∆°n 50-100x
```

### 3.2. VRAM (Video RAM)
- **ƒê·ªß VRAM**: Model load v√†o GPU ‚Üí Nhanh
- **Thi·∫øu VRAM**: Model swap ra RAM ‚Üí Ch·∫≠m 10-50x

**V√≠ d·ª•:**
- `llama3.2` c·∫ßn ~2GB VRAM
- `llama3.1:70b` c·∫ßn ~40GB VRAM

---

## 4. Y·∫øu T·ªë C·∫•u H√¨nh (·∫¢nh H∆∞·ªüng Nh·ªè)

### 4.1. Max Tokens
```python
max_tokens=1024  # Generate 1024 tokens ‚Üí Ch·∫≠m
max_tokens=256   # Generate 256 tokens ‚Üí Nhanh h∆°n 4x
```

### 4.2. Temperature
- `temperature=0.1`: Deterministic, nhanh h∆°n m·ªôt ch√∫t
- `temperature=0.9`: Random, ch·∫≠m h∆°n m·ªôt ch√∫t (kh√°c bi·ªát nh·ªè)

---

## 5. So S√°nh Th·ª±c T·∫ø Trong Code

### 5.1. Embedding Service (BGE-M3)
```python
# Model: BAAI/bge-m3 (encoder-only, ~560M params)
# Device: CUDA
# T·ªëc ƒë·ªô: ~50-200ms cho 1 query
query_embedding = embedding_service.encode_single(query)
```

**T·∫°i sao nhanh?**
- Encoder-only: X·ª≠ l√Ω song song to√†n b·ªô input
- Model nh·ªè: 560M params
- GPU: T·∫≠n d·ª•ng CUDA cores

### 5.2. LLM Service (Llama3.2)
```python
# Model: llama3.2 (decoder-only, ~3B params)
# Device: Ollama (c√≥ th·ªÉ GPU ho·∫∑c CPU)
# T·ªëc ƒë·ªô: ~1-10 gi√¢y cho 1 response
response = await llm_service.generate(messages, max_tokens=1024)
```

**T·∫°i sao ch·∫≠m h∆°n?**
- Decoder-only: Generate t·ª´ng token tu·∫ßn t·ª±
- Ph·∫£i x·ª≠ l√Ω to√†n b·ªô prompt tr∆∞·ªõc khi generate
- Output length: 1024 tokens = 1024 l·∫ßn forward pass

---

## 6. Benchmark Tham Kh·∫£o

### 6.1. Embedding Models (tokens/gi√¢y)
| Model | Size | GPU | CPU |
|-------|------|-----|-----|
| BGE-M3 | 560M | ~500-1000 | ~50-100 |
| BGE-Large | 326M | ~800-1500 | ~80-150 |

### 6.2. LLM Models (tokens/gi√¢y)
| Model | Size | GPU (RTX 3090) | CPU |
|-------|------|----------------|-----|
| Llama3.2 | 3B | ~50-100 | ~2-5 |
| Llama3.1 | 8B | ~20-40 | ~1-2 |
| Llama3.1:70b | 70B | ~5-10 | Kh√¥ng ch·∫°y ƒë∆∞·ª£c |

---

## 7. T·ªëi ∆Øu H√≥a T·ªëc ƒê·ªô

### 7.1. Ch·ªçn Model Ph√π H·ª£p
```bash
# Nhanh, ch·∫•t l∆∞·ª£ng t·ªët
ollama run llama3.2

# Nhanh h∆°n, ch·∫•t l∆∞·ª£ng gi·∫£m nh·∫π
ollama run llama3.2:q4_0
```

### 7.2. Gi·∫£m Prompt Length
```python
# Thay v√¨ top_k=20
contexts = await rag_service.retrieve_context(query, top_k=5)  # Nhanh h∆°n
```

### 7.3. Gi·∫£m Max Tokens
```python
# Thay v√¨ max_tokens=2048
response = await llm_service.generate(messages, max_tokens=512)  # Nhanh h∆°n 4x
```

### 7.4. Cache Embeddings
```python
# ƒê√£ implement trong code
redis_cache.cache_query_embedding(query, embedding)  # Tr√°nh embed l·∫°i
```

---

## 8. K·∫øt Lu·∫≠n

**T·ªëc ƒë·ªô model ph·ª• thu·ªôc v√†o:**

1. **Model (70%)**: K√≠ch th∆∞·ªõc, ki·∫øn tr√∫c, t·ªëi ∆∞u h√≥a
2. **Hardware (20%)**: GPU vs CPU, VRAM
3. **Prompt (8%)**: ƒê·ªô d√†i, ƒë·ªô ph·ª©c t·∫°p
4. **C·∫•u h√¨nh (2%)**: Max tokens, temperature

**Trong code hi·ªán t·∫°i:**
- Embedding: Nhanh (~50-200ms) - Model nh·ªè, encoder-only, GPU
- LLM: Ch·∫≠m h∆°n (~1-10s) - Decoder-only, generate tu·∫ßn t·ª±
- ƒê√£ c√≥ cache cho embeddings
- C√≥ th·ªÉ t·ªëi ∆∞u: Gi·∫£m `max_tokens`, d√πng quantization, gi·∫£m `top_k`

